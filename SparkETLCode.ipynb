{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL ASSIGNMENT: Danish ATM Transactions Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up environment variable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_232-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-210.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ATM_TRANS</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9c8c5f1190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('ATM_TRANS').master('local').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READING THE DATA\n",
    "\n",
    "##### Command to create an input schema using StructType(We recommend you to create a custom schema using the StructType class of PySpark, to avoid any data type mismatch.)\n",
    "\n",
    "##### Commands to read the data using the input schema created and verifying the data using the count function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data from the files in HDFS by a specific schema\n",
    "#creating an input schema using StructType\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, LongType\n",
    "\n",
    "fileschema= StructType([ \\\n",
    "                        StructField(\"year\",IntegerType(),True), \\\n",
    "                        StructField(\"month\",StringType(),True), \\\n",
    "                        StructField(\"day\",IntegerType(),True), \\\n",
    "                        StructField(\"weekday\",StringType(),True), \\\n",
    "                        StructField(\"hour\",IntegerType(),True), \\\n",
    "                        StructField(\"atm_status\",StringType(),True), \\\n",
    "                        StructField(\"atm_id\",IntegerType(),True), \\\n",
    "                        StructField(\"atm_manufacturer\",StringType(),True), \\\n",
    "                        StructField(\"atm_location\",StringType(),True), \\\n",
    "                        StructField(\"atm_streetname\",StringType(),True), \\\n",
    "                        StructField(\"atm_street_number\",IntegerType(),True), \\\n",
    "                        StructField(\"atm_zipcode\",IntegerType(),True), \\\n",
    "                        StructField(\"atm_lat\",DoubleType(),True), \\\n",
    "                        StructField(\"atm_lon\",DoubleType(),True), \\\n",
    "                        StructField(\"currency\",StringType(),True), \\\n",
    "                        StructField(\"card_type\",StringType(),True), \\\n",
    "                        StructField(\"transaction_amount\",IntegerType(),True), \\\n",
    "                        StructField(\"service\",StringType(),True), \\\n",
    "                        StructField(\"message_code\",StringType(),True), \\\n",
    "                        StructField(\"message_text\",StringType(),True), \\\n",
    "                        StructField(\"weather_lat\",DoubleType(),True), \\\n",
    "                        StructField(\"weather_lon\",DoubleType(),True), \\\n",
    "                        StructField(\"weather_city_id\",IntegerType(),True), \\\n",
    "                        StructField(\"weather_city_name\",StringType(),True), \\\n",
    "                        StructField(\"temp\",DoubleType(),True), \\\n",
    "                        StructField(\"pressure\",IntegerType(),True), \\\n",
    "                        StructField(\"humidity\",IntegerType(),True), \\\n",
    "                        StructField(\"wind_speed\",IntegerType(),True), \\\n",
    "                        StructField(\"wind_deg\",IntegerType(),True), \\\n",
    "                        StructField(\"rain_3h\",DoubleType(),True), \\\n",
    "                        StructField(\"clouds_all\",IntegerType(),True), \\\n",
    "                        StructField(\"weather_id\",IntegerType(),True), \\\n",
    "                        StructField(\"weather_main\",StringType(),True), \\\n",
    "                        StructField(\"weather_description\",StringType(),True), \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data using the input schema created\n",
    "\n",
    "data= spark.read.csv(\"atm_trans\",schema=fileschema)\n",
    "\n",
    "# verifying the data using the count function\n",
    "\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING DIMENSION TABLES\n",
    "\n",
    "##### Command to create a data frame for the dimension according to the target schema(dimension model) provided\n",
    "\n",
    "##### Commands to clean and transform the data: \n",
    "###### -Making sure that duplicate records are cleaned(Avoid duplicate info especially in the dimension tables.\n",
    "###### -Making sure that appropriate primary keys are present for the dimensions( You need to generate a primary key for each dimension table. For example for the 'Date' dimension one way to generate the primary key can be by adding \"date\" as the prefix  to the row number i.e. 'date1', 'date2' and so on.) \n",
    "###### -Rearranging the fields if necessary(According to the target schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the Spark Sql functions\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOCATION DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "|        atm_location|  atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|\n",
      "+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "|               Vadum|  Ellehammersvej|               43|       9430| 57.118|  9.861|\n",
      "|            Slagelse| Mariendals Alle|               29|       4200| 55.398| 11.342|\n",
      "|          Fredericia|SjÃƒÂ¦llandsgade|               33|       7000| 55.564|  9.757|\n",
      "|             Kolding|        Vejlevej|              135|       6000| 55.505|  9.457|\n",
      "|   HÃƒÂ¸rning Hallen|        Toftevej|               53|       8362| 56.091| 10.033|\n",
      "|                Aars| Himmerlandsgade|               70|       9600| 56.803|  9.518|\n",
      "|     Aarhus Lufthavn| Ny Lufthavnsvej|               24|       8560| 56.308| 10.627|\n",
      "|                 Fur|      StenÃƒÂ¸re|               19|       7884| 56.805|   9.02|\n",
      "|            Hasseris|     Hasserisvej|              113|       9000| 57.044|  9.898|\n",
      "|Intern  KÃƒÂ¸benhavn|RÃƒÂ¥dhuspladsen|               75|       1550| 55.676| 12.571|\n",
      "|      Skelagervej 15|     Skelagervej|               15|       9000| 57.023|  9.891|\n",
      "|    Intern HolbÃƒÂ¦k|     Slotsvolden|                7|       4300| 55.718| 11.704|\n",
      "|              Viborg|       Toldboden|                3|       8800| 56.448|  9.401|\n",
      "|             SÃƒÂ¦by|      Vestergade|                3|       9300| 57.334| 10.515|\n",
      "|             Aabybro|    ÃƒËœstergade|                6|       9440| 57.162|   9.73|\n",
      "|             Vodskov|      Vodskovvej|               27|       9310| 57.104| 10.027|\n",
      "|               Taars|        Bredgade|               91|       9830| 57.385| 10.116|\n",
      "|         Skive Lobby|        Adelgade|                8|       7800| 56.567|  9.027|\n",
      "|        HelsingÃƒÂ¸r|  Sct. Olai Gade|               39|       3000| 56.036| 12.612|\n",
      "|             Jebjerg|       Kirkegade|                4|       7870| 56.671|  9.013|\n",
      "+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the location dimension keeping only the necessary fields.\n",
    "#dropping duplicate values\n",
    "\n",
    "dim_location=data.select('atm_location','atm_streetname','atm_street_number','atm_zipcode','atm_lat','atm_lon').distinct()\n",
    "\n",
    "dim_location.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------------+-------------+-------+------+------+\n",
      "|location_id|            location|      streetname|street_number|zipcode|   lat|   lon|\n",
      "+-----------+--------------------+----------------+-------------+-------+------+------+\n",
      "|          1|               Vadum|  Ellehammersvej|           43|   9430|57.118| 9.861|\n",
      "|          2|            Slagelse| Mariendals Alle|           29|   4200|55.398|11.342|\n",
      "|          3|          Fredericia|SjÃƒÂ¦llandsgade|           33|   7000|55.564| 9.757|\n",
      "|          4|             Kolding|        Vejlevej|          135|   6000|55.505| 9.457|\n",
      "|          5|   HÃƒÂ¸rning Hallen|        Toftevej|           53|   8362|56.091|10.033|\n",
      "|          6|                Aars| Himmerlandsgade|           70|   9600|56.803| 9.518|\n",
      "|          7|     Aarhus Lufthavn| Ny Lufthavnsvej|           24|   8560|56.308|10.627|\n",
      "|          8|                 Fur|      StenÃƒÂ¸re|           19|   7884|56.805|  9.02|\n",
      "|          9|            Hasseris|     Hasserisvej|          113|   9000|57.044| 9.898|\n",
      "|         10|Intern  KÃƒÂ¸benhavn|RÃƒÂ¥dhuspladsen|           75|   1550|55.676|12.571|\n",
      "|         11|      Skelagervej 15|     Skelagervej|           15|   9000|57.023| 9.891|\n",
      "|         12|    Intern HolbÃƒÂ¦k|     Slotsvolden|            7|   4300|55.718|11.704|\n",
      "|         13|              Viborg|       Toldboden|            3|   8800|56.448| 9.401|\n",
      "|         14|             SÃƒÂ¦by|      Vestergade|            3|   9300|57.334|10.515|\n",
      "|         15|             Aabybro|    ÃƒËœstergade|            6|   9440|57.162|  9.73|\n",
      "|         16|             Vodskov|      Vodskovvej|           27|   9310|57.104|10.027|\n",
      "|         17|               Taars|        Bredgade|           91|   9830|57.385|10.116|\n",
      "|         18|         Skive Lobby|        Adelgade|            8|   7800|56.567| 9.027|\n",
      "|         19|        HelsingÃƒÂ¸r|  Sct. Olai Gade|           39|   3000|56.036|12.612|\n",
      "|         20|             Jebjerg|       Kirkegade|            4|   7870|56.671| 9.013|\n",
      "+-----------+--------------------+----------------+-------------+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating location_id column as primary key\n",
    "#renaming the columns as per the target schema\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id,row_number\n",
    "\n",
    "dim_location=dim_location.withColumn(\"location_id\",row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "dim_location=dim_location.withColumnRenamed('atm_location','location') \\\n",
    ".withColumnRenamed('atm_streetname','streetname') \\\n",
    ".withColumnRenamed('atm_street_number','street_number') \\\n",
    ".withColumnRenamed('atm_zipcode','zipcode') \\\n",
    ".withColumnRenamed('atm_lat','lat') \\\n",
    ".withColumnRenamed('atm_lon','lon')\n",
    "\n",
    "dim_location=dim_location.select('location_id','location','streetname','street_number','zipcode','lat','lon')\n",
    "\n",
    "dim_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying location dimension using count function\n",
    "\n",
    "dim_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- streetname: string (nullable = true)\n",
      " |-- street_number: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying schema using printSchema\n",
    "\n",
    "dim_location.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ATM DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+---------------+\n",
      "|atm_id|atm_number|atm_manufacturer|atm_location_id|\n",
      "+------+----------+----------------+---------------+\n",
      "|     1|         1|             NCR|             98|\n",
      "|     2|         2|             NCR|            103|\n",
      "|     3|         3|             NCR|             47|\n",
      "|     4|         4|             NCR|             75|\n",
      "|     5|         5|             NCR|             50|\n",
      "|     6|         6|             NCR|              3|\n",
      "|     7|         7| Diebold Nixdorf|             87|\n",
      "|     8|         8|             NCR|             21|\n",
      "|     9|         9| Diebold Nixdorf|             76|\n",
      "|    10|        10|             NCR|             26|\n",
      "|    11|        11|             NCR|             77|\n",
      "|    12|        12|             NCR|             25|\n",
      "|    13|        13|             NCR|             14|\n",
      "|    14|        14|             NCR|             95|\n",
      "|    15|        15|             NCR|             36|\n",
      "|    16|        16|             NCR|             70|\n",
      "|    17|        17|             NCR|             48|\n",
      "|    18|        18| Diebold Nixdorf|             13|\n",
      "|    19|        19|             NCR|              4|\n",
      "|    20|        20|             NCR|             96|\n",
      "+------+----------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using dict to generate foreign key atm_location_id referencing location dimension.\n",
    "#creating the ATM dimension keeping only the necessary fields\n",
    "#dropping duplicate values\n",
    "\n",
    "\n",
    "id_location_dict = dict()\n",
    "\n",
    "for location_id,location in dim_location.select('location_id','location').collect():\n",
    "  id_location_dict[location]=location_id\n",
    "\n",
    "add_location_id=udf(lambda location: id_location_dict[location])\n",
    "\n",
    "dim_atm=data.select('atm_id','atm_manufacturer','atm_location').withColumn('atm_location', add_location_id('atm_location'))\\\n",
    ".withColumnRenamed('atm_location','atm_location_id').distinct().orderBy('atm_id')\n",
    "\n",
    "#creating atm_number field\n",
    "\n",
    "dim_atm=dim_atm.withColumn('atm_number',row_number().over(Window.orderBy(monotonically_increasing_id())))\\\n",
    "                    .select('atm_id','atm_number','atm_manufacturer','atm_location_id')\n",
    "\n",
    "#casting the datatype of the columns as per the target schema\n",
    "\n",
    "dim_atm=dim_atm.withColumn('atm_location_id',col('atm_location_id').cast(IntegerType()))\\\n",
    ".withColumn('atm_number',col('atm_number').cast(StringType()))\n",
    "\n",
    "\n",
    "dim_atm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying ATM dimension using count function\n",
    "\n",
    "dim_atm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying schema using printSchema\n",
    "\n",
    "dim_atm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATE DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+----+--------+\n",
      "|year|   month|day|hour| weekday|\n",
      "+----+--------+---+----+--------+\n",
      "|2017| January|  1|   9|  Sunday|\n",
      "|2017| January|  3|   5| Tuesday|\n",
      "|2017| January|  8|  19|  Sunday|\n",
      "|2017| January| 21|   3|Saturday|\n",
      "|2017| January| 23|  21|  Monday|\n",
      "|2017|February|  2|  19|Thursday|\n",
      "|2017|February|  5|  16|  Sunday|\n",
      "|2017|February| 21|  15| Tuesday|\n",
      "|2017|   March|  2|   8|Thursday|\n",
      "|2017|   April|  2|   2|  Sunday|\n",
      "|2017|   April|  6|   8|Thursday|\n",
      "|2017|   April| 30|  10|  Sunday|\n",
      "|2017|     May|  2|   2| Tuesday|\n",
      "|2017|     May| 20|  16|Saturday|\n",
      "|2017|     May| 21|  19|  Sunday|\n",
      "|2017|    June| 27|   0| Tuesday|\n",
      "|2017|    July| 18|   9| Tuesday|\n",
      "|2017|    July| 18|  22| Tuesday|\n",
      "|2017|    July| 20|   0|Thursday|\n",
      "|2017|    July| 21|  19|  Friday|\n",
      "+----+--------+---+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the date dimension keeping only the necessary fields\n",
    "#dropping duplicate values\n",
    "\n",
    "date=data.select('year','month','day','hour','weekday').distinct()\n",
    "\n",
    "date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+---+----+--------+\n",
      "|date_id|     full_date_time|year|   month|day|hour| weekday|\n",
      "+-------+-------------------+----+--------+---+----+--------+\n",
      "|      1|2017-01-01 09:00:00|2017| January|  1|   9|  Sunday|\n",
      "|      2|2017-01-03 05:00:00|2017| January|  3|   5| Tuesday|\n",
      "|      3|2017-01-08 19:00:00|2017| January|  8|  19|  Sunday|\n",
      "|      4|2017-01-21 03:00:00|2017| January| 21|   3|Saturday|\n",
      "|      5|2017-01-23 21:00:00|2017| January| 23|  21|  Monday|\n",
      "|      6|2017-02-02 19:00:00|2017|February|  2|  19|Thursday|\n",
      "|      7|2017-02-05 16:00:00|2017|February|  5|  16|  Sunday|\n",
      "|      8|2017-02-21 15:00:00|2017|February| 21|  15| Tuesday|\n",
      "|      9|2017-03-02 08:00:00|2017|   March|  2|   8|Thursday|\n",
      "|     10|2017-04-02 02:00:00|2017|   April|  2|   2|  Sunday|\n",
      "|     11|2017-04-06 08:00:00|2017|   April|  6|   8|Thursday|\n",
      "|     12|2017-04-30 10:00:00|2017|   April| 30|  10|  Sunday|\n",
      "|     13|2017-05-02 02:00:00|2017|     May|  2|   2| Tuesday|\n",
      "|     14|2017-05-20 16:00:00|2017|     May| 20|  16|Saturday|\n",
      "|     15|2017-05-21 19:00:00|2017|     May| 21|  19|  Sunday|\n",
      "|     16|2017-06-27 00:00:00|2017|    June| 27|   0| Tuesday|\n",
      "|     17|2017-07-18 09:00:00|2017|    July| 18|   9| Tuesday|\n",
      "|     18|2017-07-18 22:00:00|2017|    July| 18|  22| Tuesday|\n",
      "|     19|2017-07-20 00:00:00|2017|    July| 20|   0|Thursday|\n",
      "|     20|2017-07-21 19:00:00|2017|    July| 21|  19|  Friday|\n",
      "+-------+-------------------+----+--------+---+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# merging date and casting to timestamp.\n",
    "\n",
    "merge_date= udf(lambda day, month, year, hour:\"{}/{}/{} {}\".format(day, datetime.strptime(month, '%B').month, year, hour))\n",
    "\n",
    "dim_date=date.withColumn('full_date_time', merge_date('day','month','year','hour'))\n",
    "dim_date=dim_date.withColumn('full_date_time', to_timestamp(\"full_date_time\", format='d/M/yyyy H'))\n",
    "\n",
    "#creating date_id as primary key\n",
    "\n",
    "dim_date=dim_date.withColumn('date_id',row_number().over(Window.orderBy(monotonically_increasing_id())))\\\n",
    ".select('date_id','full_date_time','year','month','day','hour','weekday')\n",
    "\n",
    "dim_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying date dimension using count function\n",
    "\n",
    "dim_date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying schema using printSchema\n",
    "\n",
    "dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CARD TYPE DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|card_type_id|           card_type|\n",
      "+------------+--------------------+\n",
      "|           1|     Dankort - on-us|\n",
      "|           2|              CIRRUS|\n",
      "|           3|         HÃƒÂ¦vekort|\n",
      "|           4|                VISA|\n",
      "|           5|  Mastercard - on-us|\n",
      "|           6|             Maestro|\n",
      "|           7|Visa Dankort - on-us|\n",
      "|           8|        Visa Dankort|\n",
      "|           9|            VisaPlus|\n",
      "|          10|          MasterCard|\n",
      "|          11|             Dankort|\n",
      "|          12| HÃƒÂ¦vekort - on-us|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the card type dimension keeping only the necessary fields\n",
    "# dropping duplicate values\n",
    "\n",
    "card=data.select('card_type').distinct()\n",
    "\n",
    "# creating card_type_id as primary key\n",
    "\n",
    "dim_card=card.withColumn(\"card_type_id\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\\n",
    ".select('card_type_id','card_type')\n",
    "\n",
    "dim_card.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying card type dimension using count function\n",
    "\n",
    "dim_card.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying schema using printSchema\n",
    "\n",
    "dim_card.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING FACT TABLE\n",
    "#### ATM TRANSACTION FACT TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dict to generate foreign keys in the fact table.\n",
    "\n",
    "weather_loc_dict=dict()\n",
    "date_id_dict=dict()\n",
    "cardType_id_dict=dict()\n",
    "\n",
    "\n",
    "for location_id,location in dim_location.select('location_id','location').collect():\n",
    "  weather_loc_dict[location]=location_id\n",
    "\n",
    "for card_type_id, card_type in dim_card.select('card_type_id','card_type').collect():\n",
    "  cardType_id_dict[card_type]=card_type_id\n",
    "\n",
    "for date_id, date in dim_date.select('date_id','full_date_time').collect():\n",
    "  date_id_dict[date]=date_id\n",
    "\n",
    "\n",
    "add_weather_loc_id=udf(lambda location: weather_loc_dict[location])\n",
    "add_date_id=udf(lambda date:date_id_dict[date])\n",
    "add_cardType_id=udf(lambda card_type: cardType_id_dict[card_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+\n",
      "|trans_id|atm_id|weather_loc_id|date_id|card_type_id|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main| weather_description|\n",
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+\n",
      "|       1|     1|            98|   8339|          10|    Active|     DKK|Withdrawal|              5643|        null|        null|  0.215|        92|       500|        Rain|          light rain|\n",
      "|       2|     2|           103|   8339|          10|  Inactive|     DKK|Withdrawal|              1764|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|       3|     2|           103|   8339|           4|  Inactive|     DKK|Withdrawal|              1891|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|       4|     3|            47|   8339|           4|  Inactive|     DKK|Withdrawal|              4166|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|       5|     4|            75|   8339|          10|    Active|     DKK|Withdrawal|              5153|        null|        null|    0.0|        88|       701|        Mist|                mist|\n",
      "|       6|     5|            50|   8339|          10|    Active|     DKK|Withdrawal|              3269|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|       7|     6|             3|   8339|          10|    Active|     DKK|Withdrawal|               887|        null|        null|   0.29|        92|       500|        Rain|          light rain|\n",
      "|       8|     7|            87|   8339|           5|    Active|     DKK|Withdrawal|              4626|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|       9|     8|            21|   8339|          10|    Active|     DKK|Withdrawal|               470|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|      10|     9|            76|   8339|           4|    Active|     DKK|Withdrawal|              8473|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      11|    10|            26|   8339|          11|    Active|     DKK|Withdrawal|               953|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      12|    11|            77|   8339|           8|    Active|     DKK|Withdrawal|              9346|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      13|     2|           103|   8339|           5|  Inactive|     DKK|Withdrawal|              3874|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      14|    12|            25|   8339|           5|  Inactive|     DKK|Withdrawal|              1329|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      15|    13|            14|   8339|           5|    Active|     DKK|Withdrawal|              5024|        null|        null|  1.275|        92|       500|        Rain|          light rain|\n",
      "|      16|    14|            95|   8339|           7|  Inactive|     DKK|Withdrawal|              1133|        null|        null|    0.0|        92|       300|     Drizzle|light intensity d...|\n",
      "|      17|    15|            36|   8339|          10|    Active|     DKK|Withdrawal|               594|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      18|    12|            25|   8339|           5|  Inactive|     DKK|Withdrawal|              9570|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "|      19|    16|            70|   8339|           8|  Inactive|     DKK|Withdrawal|              6068|        null|        null|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|      20|    17|            48|   8339|           4|    Active|     DKK|Withdrawal|              1630|        null|        null|   0.59|        92|       500|        Rain|          light rain|\n",
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating fact table using udf to generate foreign keys referencing to dimension tables.\n",
    "\n",
    "fact=data.select('atm_id','atm_location','year','month','day','hour','card_type','atm_status','currency','service',\\\n",
    "                 'transaction_amount','message_code','message_text','rain_3h','clouds_all','weather_id','weather_main'\\\n",
    "                 ,'weather_description')\\\n",
    ".withColumn('atm_location',add_weather_loc_id('atm_location'))\\\n",
    ".withColumn('card_type',add_cardType_id('card_type'))\\\n",
    ".withColumn('date', merge_date('day','month','year','hour')).withColumn('date', to_timestamp(\"date\", format='d/M/yyyy H'))\\\n",
    ".withColumn('date',add_date_id('date'))\\\n",
    ".withColumnRenamed('date','date_id')\\\n",
    ".withColumnRenamed('atm_location','weather_loc_id')\\\n",
    ".withColumnRenamed('card_type','card_type_id')\n",
    "\n",
    "# creating primary key trans_id\n",
    "\n",
    "fact_atm_trans=fact.withColumn('trans_id',row_number().over(Window.orderBy(monotonically_increasing_id())))\\\n",
    ".select('trans_id','atm_id','weather_loc_id','date_id','card_type_id','atm_status','currency','service','transaction_amount',\\\n",
    "        'message_code','message_text','rain_3h','clouds_all','weather_id','weather_main','weather_description')\n",
    "\n",
    "#casting the datatype of the columns as per the target schema\n",
    "\n",
    "fact_atm_trans=fact_atm_trans.withColumn('trans_id',col('trans_id').cast(LongType()))\\\n",
    ".withColumn('weather_loc_id',col('weather_loc_id').cast(IntegerType()))\\\n",
    ".withColumn('date_id',col('date_id').cast(IntegerType()))\\\n",
    ".withColumn('card_type_id',col('card_type_id').cast(IntegerType()))\n",
    "\n",
    "fact_atm_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying atm fact table using count function\n",
    "\n",
    "fact_atm_trans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: long (nullable = true)\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- weather_loc_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- card_type_id: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying schema using printSchema\n",
    "\n",
    "fact_atm_trans.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING THE DIMENSION AND FACT TABLE INTO AMAZON S3 BUCKET\n",
    "\n",
    "##### Write the DataFrames containing the dimensions and fact table directly to an S3 bucket folder. [You should create different folders on your S3 bucket for different dimensions and fact table.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading all the tables to S3\n",
    "\n",
    "dim_location.write.csv(\"s3a://atm-trans-tables/DIM_LOCATION\")\n",
    "dim_atm.write.csv(\"s3a://atm-trans-tables/DIM_ATM\")\n",
    "dim_date.write.csv(\"s3a://atm-trans-tables/DIM_DATE\")\n",
    "dim_card.write.csv(\"s3a://atm-trans-tables/DIM_CARD_TYPE\")\n",
    "fact_atm_trans.write.csv(\"s3a://atm-trans-tables/FACT_ATM_TRANS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
